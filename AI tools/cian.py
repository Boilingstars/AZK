import time
import pandas as pd
import re
import os
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from fake_useragent import UserAgent
import random

def is_valid_listing(url):
    return "sale/flat" in url or "kvartira" in url

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
def get_driver():
    options = uc.ChromeOptions()
    ua = UserAgent()
    options.add_argument(f"user-agent={ua.random}")
    options.page_load_strategy = "eager"
    options.add_argument("--headless")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return uc.Chrome(options=options)
    
def wait_get_text(driver, xpaths, timeout=10):
    for xpath in xpaths:
        try:
            element = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, xpath)))
            return element.text.strip()
        except Exception as e:
            continue  # ‚Üê –≤–∞–∂–Ω–æ! –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–∫–∞—Ç—å –¥–∞–ª—å—à–µ
    return None  # –µ—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –Ω–µ –ø–æ–¥–æ—à—ë–ª

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –ª–µ–Ω–¥–∏–Ω–≥–∞
def get_listing_links(driver, base_url, pages, max_ads_per_page):
    all_links = set()

    for page in range(1, pages + 1):
        url = base_url + f"&p={page}"
        print(f"[‚Üí] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {url}")
        driver.get(url)
        time.sleep(4)

        cards = driver.find_elements(By.XPATH, "//a[contains(@href, '/sale/flat/')]")
        for card in cards:
            href = card.get_attribute("href")
            if href and "/export/pdf/" not in href:
                link = href.split("?")[0]
                all_links.add(link)
                if len(all_links) >= pages * max_ads_per_page:
                    return list(all_links)

    return list(all_links)


# –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
def parse_ad(driver, url):
    driver.get(url)

    data = {"–°—Å—ã–ª–∫–∞": url}

    data["–¶–µ–Ω–∞"] = wait_get_text(driver,['/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[1]/div[3]/div/div[2]/span','/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[1]/div[2]/div/div[2]/span','/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[1]/div[2]/div/div[1]/span'])
    data["–¶–µ–Ω–∞ –∑–∞ –∫–≤.–º"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[3]/div/div/div[1]/span[2]'])
    data["–£—Å–ª–æ–≤–∏—è —Å–¥–µ–ª–∫–∏"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[3]/div/div/div[2]/span[2]'])
    data["–ò–ø–æ—Ç–µ–∫–∞"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[3]/div/div[1]/div[3]/div/div/div[3]/span[2]'])
    
    element = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/section/div/div/div[1]/div/h1'])
    room = re.search(r'(\d+)-–∫–æ–º–Ω', element)
    if room:
        data["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç"] = room.group(1)
    else:
        data["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç"] = "0"
  

    data["–ö–æ–º–ø–ª–µ–∫—Å"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/section/div/div/div[1]/div/div/a'])

    data["–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[3]/div/div[3]/div[1]/div[2]/div/a/span[2]/h3','/html/body/div[2]/div/div/div[2]/div[2]/div[10]/div[1]/ul/li[2]/a','/html/body/div[2]/div/div/div[2]/div[3]/div/div[3]/div/div/div[1]/div/div[2]/div[1]/div/div/a/span'])

    data["–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[1]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[1]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[1]/div[2]/span[2]'])
    data["–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[2]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[2]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/span[2]'])
    data["–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[3]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[3]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[3]/div[2]/span[2]'])
    data["–≠—Ç–∞–∂"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[4]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[4]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[4]/div[2]/span[2]'])
    data["–ì–æ–¥ —Å–¥–∞—á–∏"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[5]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[5]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[5]/div[2]/span[2]'])
    data["–î–æ–º"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[6]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[6]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[6]/div[2]/span[2]'])
    data["–û—Ç–¥–µ–ª–∫–∞"] = wait_get_text(driver, ['/html/body/div[2]/div/div/div[2]/div[2]/div[4]/div[7]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[5]/div[7]/div[2]/span[2]','/html/body/div[2]/div/div/div[2]/div[2]/div[3]/div[7]/div[2]/span[2]'])

    def get_address_parts(driver):
        parts = []
        for i in range(1, 7):  # a[1] –¥–æ a[6]
            xpath = f"/html/body/div[2]/div/div/div[2]/div[2]/section/div/div/div[2]/address/div/div/a[{i}]"
            try:
                el = driver.find_element(By.XPATH, xpath)
                text = el.text.strip()
                if text:
                    parts.append(text)
            except NoSuchElementException:
                continue
        return ", ".join(parts)

    data["–ü–æ–ª–Ω—ã–π –∞–¥—Ä–µ—Å"] = get_address_parts(driver)
    
    print("[‚úì] –°–æ–±—Ä–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ:", data)
    return data


# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫
def run_parser(base_urls, ads_per_site,pages):
    driver = get_driver()
    results = []

    try:
        for site_name, base_url in base_urls.items():
            print(f"\n‚ñ∂ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {site_name}")
            links = get_listing_links(driver, base_url, pages, ads_per_site)
            total = len(links)
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ {total} —Å—Å—ã–ª–æ–∫")

            for i, url in enumerate(links, 1):
                print(f"\nüìÑ [{i}/{total}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")
                try:
                    data = parse_ad(driver, url)
                    results.append(data)

                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ. –°–æ–±—Ä–∞–Ω–æ {len(results)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø–∏—Å–µ–π
                    if len(results) % 5 == 0:
                        filename = f"–Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∏({len(results)}).xlsx"
                        df = pd.DataFrame(results)
                        df.to_excel(filename, index=False)
                        print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")

                except Exception as e:
                    print(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")

    finally:
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        filename = f"–Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∏({len(results)}).xlsx"
        df = pd.DataFrame(results)
        df.to_excel(filename, index=False)
        print(f"\nüèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª: {filename}")
        driver.quit()

# URL-–ª–µ–Ω–¥–∏–Ω–≥–∏ –Ω–æ–≤–æ—Å—Ç—Ä–æ–µ–∫
base_urls = {"–¶–ò–ê–ù":"https://krasnodar.cian.ru/cat.php?deal_type=sale&engine_version=2&object_type%5B0%5D=2&offer_type=flat&region=4820"}

if __name__ == "__main__":
    run_parser(base_urls, ads_per_site=30, pages= 10)